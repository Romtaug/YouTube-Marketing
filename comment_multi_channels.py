# comment_multi_channels.py
# D√©pendances: google-api-python-client google-auth-oauthlib isodate
# pip install google-api-python-client google-auth-oauthlib isodate

from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import google.oauth2.credentials as oauth2
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

import os, json, isodate, time, random, datetime as _dt
from typing import List, Tuple, Optional, Dict, Any

# ===============================
# CONFIG AUTH
# ===============================
CLIENT_ID     = os.getenv("YT_CLIENT_ID")      # via secrets/ENV
CLIENT_SECRET = os.getenv("YT_CLIENT_SECRET")  # via secrets/ENV
PORT          = 8080
SCOPES        = ["https://www.googleapis.com/auth/youtube.force-ssl"]
TOKEN_FILE    = "token_comment.json"

# ===============================
# COMMENT BUILDER (r√©aliste & variable)
# ===============================
SUB_LINK = "https://youtube.com/@MrPlavon?sub_confirmation=1"
_utm = _dt.datetime.utcnow().strftime("%Y%m%d")
SUB_LINK_UTM = f"{SUB_LINK}&utm_source=yt_comments&utm_medium=bot&utm_campaign=auto_{_utm}"

# Ratios (ajuste si tu veux √™tre plus/moins agressif)
INCLUDE_LINK_RATIO  = 0.25   # % de commentaires avec le lien d‚Äôabo
SELF_MENTION_RATIO  = 0.45   # % avec mention ‚Äúje fais du contenu similaire‚Äù
EMOJI_RATIO         = 0.5    # % avec 1‚Äì3 emojis
MAX_EMOJIS          = 3
EMOJI_POOL = ["üî•","üöÄ","üëè","üí°","üéØ","üìà","üëå","üôå","‚ú®"]

PREFIXES = ["", "Franchement, ", "Honn√™tement, ", "Pour √™tre sinc√®re, "]
CLOSERS  = ["", " Merci pour le partage.", " H√¢te de voir la suite.", " Beau taf."]

SELF_MENTIONS = [
    " Je fais du contenu dans la m√™me vibe sur ma cha√Æne.",
    " Je publie des analyses similaires, √ßa peut vous int√©resser.",
    " Je teste des formats proches sur ma cha√Æne si √ßa te parle.",
    " Je poste des d√©briefs similaires de mon c√¥t√©.",
]

VIDEO_TEMPLATES = [
    "{p}Super clair et concret, j‚Äôai pris 2‚Äì3 id√©es actionnables.{self}{link}{e}{c}",
    "{p}Bon rythme et explications simples, √ßa donne envie de tester direct.{self}{link}{e}{c}",
    "{p}J‚Äôai bien aim√© la partie strat√©gie, √ßa m‚Äôa fait r√©fl√©chir.{self}{link}{e}{c}",
    "{p}Z√©ro bla-bla, juste l‚Äôessentiel. tu feras un suivi sur ce sujet ?{self}{link}{e}{c}",
    "{p}Merci pour la valeur, j‚Äôapplique √ßa d√®s aujourd‚Äôhui.{self}{link}{e}{c}",
    "{p}Bonne synth√®se, curieux d‚Äôune version plus avanc√©e.{self}{link}{e}{c}",
]

SHORT_TEMPLATES = [
    "{p}Format efficace, message clair en 60s, j‚Äôaime beaucoup.{self}{link}{e}{c}",
    "{p}Bonne punchline, action simple √† faire maintenant.{self}{link}{e}{c}",
    "{p}Court et utile, parfait pour s‚Äôy mettre sans se perdre.{self}{link}{e}{c}",
    "{p}Tr√®s direct, √ßa motive √† passer √† l‚Äôaction tout de suite.{self}{link}{e}{c}",
    "{p}Petite p√©pite, je garde l‚Äôid√©e pour la semaine.{self}{link}{e}{c}",
]

def _maybe_link() -> str:
    return f" (si √ßa t‚Äôint√©resse : {SUB_LINK_UTM})" if random.random() < INCLUDE_LINK_RATIO else ""

def _maybe_emojis() -> str:
    if random.random() > EMOJI_RATIO:
        return ""
    k = random.randint(1, MAX_EMOJIS)
    return " " + "".join(random.sample(EMOJI_POOL, k))

def _maybe_self_mention() -> str:
    return random.choice(SELF_MENTIONS) if random.random() < SELF_MENTION_RATIO else ""

def _polish(text: str, max_len: int = 230) -> str:
    t = " ".join(text.split())
    return (t[: max_len - 1] + "‚Ä¶") if len(t) > max_len else t

def make_comment_for_video() -> str:
    base = random.choice(VIDEO_TEMPLATES)
    txt = base.format(
        p=random.choice(PREFIXES), self=_maybe_self_mention(),
        link=_maybe_link(), e=_maybe_emojis(), c=random.choice(CLOSERS),
    )
    return _polish(txt)

def make_comment_for_short() -> str:
    base = random.choice(SHORT_TEMPLATES)
    txt = base.format(
        p=random.choice(PREFIXES), self=_maybe_self_mention(),
        link=_maybe_link(), e=_maybe_emojis(), c=random.choice(CLOSERS),
    )
    return _polish(txt)

# ===============================
# LISTE CHA√éNES (mode vendredi)
# ===============================
CHANNEL_TARGETS: List[str] = [
    "Yomi Denzel", "Theophile Eliet", "@oussamaammaroff", "Enzo Honore", "@legendmedia",
    "Olivier Roland", "Marketing Mania", "Business Dynamite", "Le Marketeur Fran√ßais",
    "Tugan Bara", "Antoine BM", "Jean Riviere", "Alexandre Roth", "@Lo√Øcbourget",
    "Money Radar", "Yann Darwin", "Christopher Wangen", "Pierre Ollier", "@Objectif-Renta",
    "Greenbull Campus", "Finary", "S'investir - Matthieu Louvet", "Epargnant 3.0",
    "Riche √† 30 ans", "Investir Simple", "Le Revenu", "Capital",
    "David Laroche", "Franck Nicolas", "Alexandre Cormont", "Idriss Aberkane",
    "Jean Laval", "Laurent Chenot", "Emmanuel Fredenrich", "Anthony Nevo",
    "Pauline Laigneau", "Mind Parachutes", "@ego_one", "@LaMenaceVlogs",
    "Thinkerview", "Heu?reka", "Draw My Economy", "Institut des Libertes",
    "Les Echos", "BFM Business", "Zone Bourse", "IG France", "TV Finance",
    "Hasheur", "Cryptoast", "Journal du Coin", "Thami Kabbaj", "Young Trader Wealth",
    "@singeexplique", "@timotheemoiroux", "S√©bastienKoubar", "@Shubham_Sharma",
    "@sanspermissionpodcast", "@monsieurrodolphe1", "@MatthieuLouvet",
    "@MoneyRadarAvis", "@moneyradarcrypto", "@amistory", "@GaspardG", "@hugodecrypteactus"
]

# ===============================
# RECHERCHE TH√àMES (autres jours)
# ===============================
THEME_QUERIES: List[str] = [
    "actualit√© financi√®re", "dette", "IA", "Agent IA", "trading en france", "investissement", "√©conomie", "stoicisme",
    "business en ligne", "crypto en france", "immobilier", "ing√©nieur data", "bourse", "lib√©ralisme", "pragmatisme",
    "entrepreneuriat", "dev perso", "patrimoine", "gagnant", "fintech", "n√©obanque", "vid√©o de motivation", "aller √† Duba√Ø",
    "revenus passifs", "ind√©pendance financi√®re", "fiscalit√© france", "assurance vie", "ETF", "obligations fran√ßaises",
    "intelligence artificielle", "automatisation no-code", "growth hacking", "side hustle", "start-up fran√ßaise", "podcast business",
    "blockchain en france", "web3 france", "productivit√© quotidienne", "habitudes millionnaires", "mindset d'entrepreneur",
    "r√©silience mentale", "travail au luxembourg", "expatriation suisse", "freelance data", "consultant strat√©gie",
    "intelligence financi√®re", "revenus en ligne", "cryptomonnaies", "gestion du temps", "succ√®s entrepreneurial"
]

SEARCH_RELEVANCE_LANG = "fr"
SEARCH_REGION_CODE    = "FR"   # commente si trop restrictif
SEARCH_PUBLISHED_AFTER_DAYS = 30  # √©largit la moisson
SEARCH_PAGE_LIMIT     = 20       # jusqu‚Äô√† ~1000 r√©sultats bruts/req
NEED_VIDEOS           = 45       # >= 60s
NEED_SHORTS           = 45       # < 60s

# S√©curit√© & anti-spam
MAX_COMMENTS_PER_RUN  = 100
SLEEP_MIN, SLEEP_MAX  = 1.2, 3.0   # jitter entre commentaires

# Override pour tester le mode ‚Äúvendredi‚Äù
IS_FRIDAY_OVERRIDE = os.getenv("FORCE_FRIDAY") == "1"

# ===============================
# HELPERS R√âSEAU / API ROBUSTES
# ===============================
TRANSIENT_REASONS = {
    "rateLimitExceeded", "quotaExceeded", "backendError",
    "internalError", "badGateway", "timeout", "dailyLimitExceeded",
}

def _reason_of_http_error(e: HttpError) -> str:
    try:
        data = json.loads(e.content.decode("utf-8"))
        errs = data.get("error", {}).get("errors", [])
        if errs:
            return f"{errs[0].get('reason')} ‚Äì {errs[0].get('message')}"
        return data.get("error", {}).get("message", str(e))
    except Exception:
        return str(e)

def _exec(req, retries: int = 4, delay: float = 1.0):
    """Ex√©cute une requ√™te YouTube avec retry/backoff, g√®re 4xx/5xx proprement."""
    for i in range(retries):
        try:
            return req.execute()
        except HttpError as e:
            status = getattr(e, "status_code", None) or getattr(e.resp, "status", None)
            reason = _reason_of_http_error(e)
            # 4xx d√©finitifs (souvent), mais certaines raisons peuvent √™tre transientes
            if status and 400 <= int(status) < 500 and not any(r in reason for r in TRANSIENT_REASONS):
                if i == retries - 1:
                    print(f"[ERROR] HTTP {status} (definitif): {reason}")
                    raise
                time.sleep(delay * (i + 1))
            else:
                # 5xx / transients (ou 4xx avec reason transiente)
                if i == retries - 1:
                    print(f"[ERROR] HTTP {status}: {reason}")
                    raise
                sleep_s = delay * (i + 1) * (1.5 + random.random())
                print(f"[WARN] HTTP {status}: {reason} ‚Üí retry in {sleep_s:.1f}s")
                time.sleep(sleep_s)

def _is_live_or_upcoming(snippet: Dict[str, Any], live_streaming_details: Dict[str, Any]) -> bool:
    # D√©tection fiable live / upcoming
    if live_streaming_details and any(k in live_streaming_details for k in ("actualStartTime", "scheduledStartTime")):
        return True
    lbc = (snippet or {}).get("liveBroadcastContent")
    return lbc in ("live", "upcoming")

def _is_kids(status: Dict[str, Any]) -> bool:
    return bool((status or {}).get("madeForKids"))

# ===============================
# AUTH
# ===============================
def _load_token(path: str, scopes: List[str]) -> Optional[oauth2.Credentials]:
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "refresh_token" not in data:
            return None
        return oauth2.Credentials.from_authorized_user_info(data, scopes)
    except Exception:
        return None

def auth_youtube():
    creds = _load_token(TOKEN_FILE, SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            if not CLIENT_ID or not CLIENT_SECRET:
                raise RuntimeError("CLIENT_ID/CLIENT_SECRET manquants (ENV YT_CLIENT_ID / YT_CLIENT_SECRET).")
            client_config = {
                "installed": {
                    "client_id": CLIENT_ID,
                    "client_secret": CLIENT_SECRET,
                    "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                    "token_uri": "https://oauth2.googleapis.com/token",
                    "redirect_uris": [f"http://localhost:{PORT}/"]
                }
            }
            flow = InstalledAppFlow.from_client_config(client_config, SCOPES)
            creds = flow.run_local_server(
                port=PORT, access_type="offline", prompt="consent",
                include_granted_scopes=False
            )
        with open(TOKEN_FILE, "w", encoding="utf-8") as f:
            f.write(creds.to_json())
    return build("youtube", "v3", credentials=creds)

# ===============================
# HELPERS API
# ===============================
def iso_to_seconds(iso: str) -> int:
    return int(isodate.parse_duration(iso).total_seconds())

def comment(yt, video_id: str, text: str):
    body = {"snippet": {"videoId": video_id, "topLevelComment": {"snippet": {"textOriginal": text}}}}
    return _exec(yt.commentThreads().insert(part="snippet", body=body))

def resolve_uploads_playlist(yt, target: str) -> Tuple[str, str, str]:
    """
    Retourne: (uploads_playlist_id, channel_title, channel_id)
    G√®re @handle ou recherche par nom.
    """
    t = (target or "").strip()
    if not t:
        raise ValueError("Cible cha√Æne vide.")
    if t.startswith("@"):
        h = t[1:]
        ch = _exec(yt.channels().list(part="id,snippet,contentDetails", forHandle=h))
        items = ch.get("items", [])
        if not items:
            raise ValueError(f"Handle introuvable: {t}")
        c = items[0]
        upl = (c.get("contentDetails") or {}).get("relatedPlaylists", {}).get("uploads")
        if not upl:
            raise ValueError(f"Pas de playlist 'uploads' pour {t}")
        return upl, (c.get("snippet") or {}).get("title", t), c.get("id", "")
    # recherche par nom
    sr = _exec(yt.search().list(part="snippet", q=t, type="channel", maxResults=1))
    sitems = sr.get("items", [])
    if not sitems:
        raise ValueError(f"Cha√Æne introuvable via recherche: {t}")
    channel_id = sitems[0]["snippet"]["channelId"]
    ch = _exec(yt.channels().list(part="id,snippet,contentDetails", id=channel_id))
    items = ch.get("items", [])
    if not items:
        raise ValueError(f"Impossible de charger la cha√Æne: {t} ({channel_id})")
    c = items[0]
    upl = (c.get("contentDetails") or {}).get("relatedPlaylists", {}).get("uploads")
    if not upl:
        raise ValueError(f"Pas de playlist 'uploads' pour {t}")
    return upl, (c.get("snippet") or {}).get("title", t), c.get("id", channel_id)

def find_last_video_and_short(yt, uploads_id: str) -> Tuple[Optional[Tuple[str, str]], Optional[Tuple[str, str]]]:
    """
    R√©cup√®re la derni√®re vid√©o >=60s ET le dernier short <60s,
    en ignorant lives/upcoming, madeForKids et vid√©os sans contentDetails.duration.
    Retourne: ( (video_id, title) | None, (short_id, title) | None )
    """
    try:
        items = _exec(yt.playlistItems().list(
            part="contentDetails", playlistId=uploads_id, maxResults=50
        )).get("items", [])
    except HttpError as e:
        print(f"[WARN] Playlist introuvable/priv√©e '{uploads_id}': {_reason_of_http_error(e)}")
        return None, None

    if not items:
        return None, None

    video_ids = []
    for it in items:
        cd = it.get("contentDetails") or {}
        vid = cd.get("videoId")
        if vid:
            video_ids.append(vid)

    if not video_ids:
        return None, None

    details: Dict[str, Dict[str, Any]] = {}
    for i in range(0, len(video_ids), 50):
        chunk = ",".join(video_ids[i:i+50])
        resp = _exec(yt.videos().list(
            part="contentDetails,snippet,status,liveStreamingDetails",
            id=chunk
        ))
        for it in resp.get("items", []):
            vid = it.get("id")
            cdet = it.get("contentDetails") or {}
            sni  = it.get("snippet") or {}
            stat = it.get("status") or {}
            lsd  = it.get("liveStreamingDetails") or {}

            dur_iso = cdet.get("duration")
            if not dur_iso:
                # Ex: vid√©o priv√©e/members-only/supprim√©e/restrictions ‚Üí pas de duration
                print(f"[WARN] Vid√©o sans duration, ignor√©e: id={vid} title={sni.get('title')!r}")
                continue

            try:
                dur_s = iso_to_seconds(dur_iso)
            except Exception:
                print(f"[WARN] Duration illisible {dur_iso!r} pour id={vid}, ignor√©e.")
                continue

            # Filtre live/upcoming/kids
            if _is_live_or_upcoming(sni, lsd) or _is_kids(stat):
                continue

            details[vid] = {
                "duration_s": dur_s,
                "title": sni.get("title", ""),
                "snippet": sni,
                "status": stat,
                "lsd": lsd,
            }

    if not details:
        return None, None

    last_video: Optional[Tuple[str, str]] = None
    last_short: Optional[Tuple[str, str]] = None

    # On respecte l‚Äôordre de la playlist (d√©j√† du plus r√©cent au moins)
    for vid in video_ids:
        d = details.get(vid)
        if not d:
            continue
        if d["duration_s"] >= 60 and last_video is None:
            last_video = (vid, d["title"])
        elif d["duration_s"] < 60 and last_short is None:
            last_short = (vid, d["title"])
        if last_video and last_short:
            break

    return last_video, last_short

# NEW: helper pour construire l'URL vid√©o compl√®te
def video_url(video_id: str) -> str:
    return f"https://www.youtube.com/watch?v={video_id}"

# ===============================
# RECHERCHE TH√àMES ‚Äì multi-requ√™tes & 2 passes
# ===============================
def search_theme_collect(yt, query_or_queries, need_videos=50, need_shorts=50) -> Tuple[List[Tuple[str,str]], List[Tuple[str,str]]]:
    import datetime as _dt
    queries = query_or_queries if isinstance(query_or_queries, list) else [query_or_queries]

    def _collect(order_mode: str, use_published_after: bool = True):
        videos: List[Tuple[str,str]] = []
        shorts: List[Tuple[str,str]] = []
        seen_ids = set()
        published_after_iso = None
        if use_published_after:
            ts = time.time() - SEARCH_PUBLISHED_AFTER_DAYS * 86400
            published_after_iso = _dt.datetime.utcfromtimestamp(ts).strftime("%Y-%m-%dT%H:%M:%SZ")

        for q in queries:
            page_token = None
            page_count = 0
            while page_count < SEARCH_PAGE_LIMIT and (len(videos) < need_videos or len(shorts) < need_shorts):
                params = dict(
                    part="id,snippet", q=q, type="video", maxResults=50,
                    order=order_mode, relevanceLanguage=SEARCH_RELEVANCE_LANG,
                )
                # Optionnel: commente la ligne ci-dessous si trop restrictif
                params["regionCode"] = SEARCH_REGION_CODE
                if published_after_iso:
                    params["publishedAfter"] = published_after_iso
                if page_token:
                    params["pageToken"] = page_token

                res = _exec(yt.search().list(**params))
                items = res.get("items", [])
                ids = [it["id"]["videoId"] for it in items if it.get("id", {}).get("kind") == "youtube#video" and it["id"].get("videoId")]
                if not ids:
                    break

                det = _exec(yt.videos().list(
                    part="contentDetails,snippet,status,liveStreamingDetails",
                    id=",".join(ids)
                ))
                for it in det.get("items", []):
                    vid = it.get("id")
                    if not vid or vid in seen_ids:
                        continue

                    cdet = it.get("contentDetails") or {}
                    sni  = it.get("snippet") or {}
                    stat = it.get("status") or {}
                    lsd  = it.get("liveStreamingDetails") or {}

                    dur_iso = cdet.get("duration")
                    if not dur_iso:
                        # Vid√©os sans duration (priv√©e/members-only etc.)
                        continue
                    try:
                        dur = iso_to_seconds(dur_iso)
                    except Exception:
                        continue

                    # Filtrer lives / upcoming / kids
                    if _is_live_or_upcoming(sni, lsd) or _is_kids(stat):
                        continue

                    title = sni.get("title", "")
                    if dur < 60 and len(shorts) < need_shorts:
                        shorts.append((vid, title)); seen_ids.add(vid)
                    elif dur >= 60 and len(videos) < need_videos:
                        videos.append((vid, title)); seen_ids.add(vid)
                    if len(videos) >= need_videos and len(shorts) >= need_shorts:
                        break

                page_token = res.get("nextPageToken")
                page_count += 1
                if not page_token:
                    break
        return videos, shorts

    # Pass 1 : r√©cents + tri par date
    V1, S1 = _collect(order_mode="date", use_published_after=True)
    if len(V1) >= need_videos and len(S1) >= need_shorts:
        return V1[:need_videos], S1[:need_shorts]

    # Pass 2 : √©largi (sans publishedAfter) + tri par pertinence
    V2, S2 = _collect(order_mode="relevance", use_published_after=False)

    # Merge & tronque
    def _merge_take(A: List[Tuple[str,str]], B: List[Tuple[str,str]], n: int):
        out, seen = [], set()
        for x in A + B:
            if x[0] in seen: continue
            out.append(x); seen.add(x[0])
            if len(out) >= n: break
        return out

    videos = _merge_take(V1, V2, need_videos)
    shorts = _merge_take(S1, S2, need_shorts)
    return videos, shorts

# ===============================
# MAIN
# ===============================
def main():
    yt = auth_youtube()

    # Vendredi ? (respecte TZ du runner si tu l‚Äôas fix√©e, ex: Europe/Paris)
    weekday = time.localtime().tm_wday  # 0=lundi, 4=vendredi
    is_friday = (weekday == 4) or IS_FRIDAY_OVERRIDE

    total_comments = 0
    already_done = set()

    def _sleep():
        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))

    if is_friday:
        print("Mode: listes de YouTubers (vendredi)")
        vids_needed, shorts_needed = 50, 50

        for target in CHANNEL_TARGETS:
            if (vids_needed <= 0 and shorts_needed <= 0) or total_comments >= MAX_COMMENTS_PER_RUN:
                break
            try:
                uploads_id, channel_title, _ = resolve_uploads_playlist(yt, target)
            except Exception as e:
                print(f"[WARN] R√©solution √©chou√©e '{target}': {e}")
                continue

            last_video, last_short = find_last_video_and_short(yt, uploads_id)

            if last_video and vids_needed > 0:
                vid, title = last_video
                if vid not in already_done:
                    try:
                        resp = comment(yt, vid, make_comment_for_video())
                        print(f"[OK] Liste: video '{title}' -> {video_url(vid)}  (comment: {resp.get('id')})")
                        total_comments += 1; vids_needed -= 1; already_done.add(vid)
                        _sleep()
                    except HttpError as e:
                        print(f"[WARN] Echec com video '{title}' ({vid}): {_reason_of_http_error(e)}")

            if last_short and shorts_needed > 0 and total_comments < MAX_COMMENTS_PER_RUN:
                sid, stitle = last_short
                if sid not in already_done:
                    try:
                        resp = comment(yt, sid, make_comment_for_short())
                        print(f"[OK] Liste: short '{stitle}' -> {video_url(sid)}  (comment: {resp.get('id')})")
                        total_comments += 1; shorts_needed -= 1; already_done.add(sid)
                        _sleep()
                    except HttpError as e:
                        print(f"[WARN] Echec com short '{stitle}' ({sid}): {_reason_of_http_error(e)}")

        print(f"Reste (apr√®s listes): videos={vids_needed}, shorts={shorts_needed}")

        if (vids_needed > 0 or shorts_needed > 0) and total_comments < MAX_COMMENTS_PER_RUN:
            print("Compl√©ment par recherche de th√®me pour atteindre 50/50.")
            vlist, slist = search_theme_collect(yt, THEME_QUERIES, vids_needed, shorts_needed)

            for vid, title in vlist:
                if total_comments >= MAX_COMMENTS_PER_RUN: break
                if vid in already_done: continue
                try:
                    resp = comment(yt, vid, make_comment_for_video())
                    print(f"[OK] Th√®me: video '{title}' -> {video_url(vid)}  (comment: {resp.get('id')})")
                    total_comments += 1; already_done.add(vid)
                    _sleep()
                except HttpError as e:
                    print(f"[WARN] Echec com th√®me video '{title}' ({vid}): {_reason_of_http_error(e)}")

            for sid, stitle in slist:
                if total_comments >= MAX_COMMENTS_PER_RUN: break
                if sid in already_done: continue
                try:
                    resp = comment(yt, sid, make_comment_for_short())
                    print(f"[OK] Th√®me: short '{stitle}' -> {video_url(sid)}  (comment: {resp.get('id')})")
                    total_comments += 1; already_done.add(sid)
                    _sleep()
                except HttpError as e:
                    print(f"[WARN] Echec com th√®me short '{stitle}' ({sid}): {_reason_of_http_error(e)}")

    else:
        print("Mode: recherche par th√®me (hors vendredi)")
        vlist, slist = search_theme_collect(yt, THEME_QUERIES, NEED_VIDEOS, NEED_SHORTS)

        for vid, title in vlist:
            if total_comments >= MAX_COMMENTS_PER_RUN: break
            try:
                resp = comment(yt, vid, make_comment_for_video())
                print(f"[OK] Th√®me: video '{title}' -> {video_url(vid)}  (comment: {resp.get('id')})")
                total_comments += 1
                _sleep()
            except HttpError as e:
                print(f"[WARN] Echec com th√®me video '{title}' ({vid}): {_reason_of_http_error(e)}")

        for sid, stitle in slist:
            if total_comments >= MAX_COMMENTS_PER_RUN: break
            try:
                resp = comment(yt, sid, make_comment_for_short())
                print(f"[OK] Th√®me: short '{stitle}' -> {video_url(sid)}  (comment: {resp.get('id')})")
                total_comments += 1
                _sleep()
            except HttpError as e:
                print(f"[WARN] Echec com th√®me short '{stitle}' ({sid}): {_reason_of_http_error(e)}")

    print(f"\nTotal de commentaires post√©s : {total_comments}")

# ===============================
# RUN
# ===============================
if __name__ == "__main__":
    main()



